{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling of Ubuntu Dataset\n",
    "- Time to complete: 7 hours\n",
    "\n",
    "- In this notebook. we would like to do some topic modelling of the Ubuntu Dataset, which is a large corpus of multi-turn chat dialogues between users and tech supports for Ubuntu OS related issues.\n",
    "\n",
    "- We'll do the following:\n",
    "    - Vectorize a streamed corpus\n",
    "    - Run topic modelling on streamed vectors, using gensim's Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) algoirthms. \n",
    "    - Determine Top 10 Topics of our training set ('dialogs/4' folder)\n",
    "    - Evaluate topic models on a test set (30K files in 'dialogs/5' folder)\n",
    "    - Create a Topic Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "import glob\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all pathnames for chat dialogues in data/dialog/4 directory\n",
    "chat_dialogues = glob.glob('data/dialogs/4/*.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/dialogs/4/1.tsv',\n",
       " 'data/dialogs/4/10.tsv',\n",
       " 'data/dialogs/4/100.tsv',\n",
       " 'data/dialogs/4/1000.tsv',\n",
       " 'data/dialogs/4/10000.tsv',\n",
       " 'data/dialogs/4/100000.tsv',\n",
       " 'data/dialogs/4/100001.tsv',\n",
       " 'data/dialogs/4/100002.tsv',\n",
       " 'data/dialogs/4/100003.tsv',\n",
       " 'data/dialogs/4/100004.tsv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_dialogues[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Example Converation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def read_tsv_file(chat_file):\n",
    "    \"\"\"\n",
    "    Extract text from each row in the chat log file\n",
    "    and return a list of dialogue containing the\n",
    "    entire conversation.\n",
    "    \"\"\"\n",
    "    dialogue = []\n",
    "    with open(chat_file) as tsv_file:\n",
    "        reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            dialogue.append(str(row[3]).strip())\n",
    "    return dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ahhh how to get fix nautilus from going slow in hoary?', 'when you upgraded, did you install gamin?', 'what does that do?', 'if you have an apt or sed pacakge there...']\n"
     ]
    }
   ],
   "source": [
    "chat_example = read_tsv_file(chat_dialogues[2])\n",
    "print chat_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ubuntu Corpus\n",
    "\n",
    "Let's stream over an entire file directory of chat dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def extract_text_from_chat_dialogue(chat_file):\n",
    "    chat_dialogue = read_tsv_file(chat_file)\n",
    "    return ' '.join(chat_dialogue)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(strip_punctuation(text.strip())) if token not in STOPWORDS]\n",
    "\n",
    "def iter_ubuntu(chat_files):\n",
    "    for chat_file in chat_files[1:]:\n",
    "        text = extract_text_from_chat_dialogue(chat_file)\n",
    "        tokens = tokenize(text)\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 1\n",
      "Tokens: [u'add', u'lines', u'xinetd', u'hup', u'load', u'server', u'hrm', u'installed', u'hotwayd', u'giving']\n",
      "\n",
      "Document: 2\n",
      "Tokens: [u'ahhh', u'get', u'fix', u'nautilus', u'going', u'slow', u'hoary', u'upgraded', u'install', u'gamin']\n",
      "\n",
      "Document: 3\n",
      "Tokens: [u'anyone', u'use', u'xorg', u'edgers', u'ppa', u'curiousity', u'ppas', u'unsupported', u'rd', u'party']\n",
      "\n",
      "Document: 4\n",
      "Tokens: [u'ssh', u'encryption', u'channels', u'available', u'freenode', u'connect', u'ssl', u'irc', u'freenode', u'net']\n",
      "\n",
      "Document: 5\n",
      "Tokens: [u'installed', u'ubuntu', u'server', u'would', u'need', u'install', u'get', u'graphical', u'application', u'run']\n",
      "\n",
      "Document: 6\n",
      "Tokens: [u'serious', u'wierd', u'problem', u'file', u'etc', u'resolv', u'conf', u'cannot', u'accessed', u'removed']\n",
      "\n",
      "Document: 7\n",
      "Tokens: [u'boot', u'command', u'line', u'grub', u'command', u'line', u'actual', u'ubuntu', u'command', u'line']\n",
      "\n",
      "Document: 8\n",
      "Tokens: [u'ffs', u'enough', u'people', u'problem', u'upgrading', u'get', u'error', u'downloading', u'anyone', u'help']\n",
      "\n",
      "Document: 9\n",
      "Tokens: [u'hi', u'enable', u'css', u'django', u'apache', u'web', u'server', u'one', u'help', u'yes']\n",
      "\n",
      "Document: 10\n",
      "Tokens: [u'another', u'location', u'network', u'configs', u'apart', u'interfaces', u'file', u'coiuld', u'bringing', u'int']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = iter_ubuntu(chat_dialogues)\n",
    "for i, tokens in enumerate(itertools.islice(stream, 10)):\n",
    "    print \"Document: {}\".format(i+1)\n",
    "    print \"Tokens: {}\".format(tokens[:10])\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "\n",
    "We need a mapping of raw text tokens to numerical tokens becuase most machine learning algorithms rely on numerical libraries indexed by integers, rather than by strings, and have to know the vector/matrix dimensionality in advance.\n",
    "\n",
    "The mapping can be constructed automatically by giving Dictionary class a stream of tokenized documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "chat_stream = (tokens for tokens in tqdm(iter_ubuntu(chat_dialogues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269022it [02:00, 2241.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 16.3 s, total: 1min 29s\n",
      "Wall time: 1min 59s\n",
      "Dictionary(124597 unique tokens: [u'fawn', u'unsupportable', u'fawk', u'mdraid', u'userscripts']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time id2word_ubuntu = gensim.corpora.Dictionary(chat_stream)\n",
    "print id2word_ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dictionary mapping now contains all words that appeared in the corpus, along with how many times they appeared. Let's filter out both very infrequent words (stopwords) and very frequent words to clear up resources as well as remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14492 unique tokens: [u'fawn', u'adviced', u'fucked', u'libmad', u'icmp']...)\n"
     ]
    }
   ],
   "source": [
    "id2word_ubuntu.filter_extremes(no_below=10, no_above=0.1)\n",
    "print id2word_ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "\n",
    "A streamed corpus and a dictionary is all we need to create bag-of-words vectors.\n",
    "\n",
    "Let's wrap the entire dialogue directory, as a stream of bag-of-word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UbuntuCorpus(object):\n",
    "    def __init__(self, dialogue_directory, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Ubuntu chat dialogues \n",
    "        from directory `dialogue directory`. Yield each \n",
    "        document in turn, as a list of tokens (unicode strings).\n",
    "        \"\"\"        \n",
    "        self.directory = dialogue_directory\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        chat_files = glob.glob(self.directory + '/*.tsv')\n",
    "        for tokens in tqdm(itertools.islice(iter_ubuntu(chat_files), self.clip_docs)):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "            \n",
    "    def len(self):\n",
    "        return self.clip_docs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 2), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ubuntu_corpus = UbuntuCorpus('data/dialogs/4', id2word_ubuntu)\n",
    "vector = next(iter(ubuntu_corpus))\n",
    "print vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'giving', 2)\n"
     ]
    }
   ],
   "source": [
    "# what is the most common word in that first article?\n",
    "most_index, most_count = max(vector, key=lambda (word_index, count): count)\n",
    "print(id2word_ubuntu[most_index], most_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store all those bag-of-words vectors into a file, so we don't have to parse through the chat logs\n",
    "every time over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "269022it [02:30, 1789.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 21.5 s, total: 1min 57s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%time gensim.corpora.MmCorpus.serialize('./data/gensim_models/ubuntu_bow.mm', ubuntu_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(269022 documents, 14492 features, 4560490 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "ubuntu_corpus = gensim.corpora.MmCorpus('./data/gensim_models/ubuntu_bow.mm')\n",
    "print ubuntu_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling: Semantic Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 224 ms, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# use fewer documents during experimentation (LDA is slow) to see if topic_extraction is working well or not\n",
    "\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(ubuntu_corpus, 10000)\n",
    "\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=25, id2word=id2word_ubuntu, passes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 55s, sys: 3.06 s, total: 21min 58s\n",
      "Wall time: 22min 2s\n"
     ]
    }
   ],
   "source": [
    "# Use all 270K+ documents for training our LDA model that will extract 25 latent topics\n",
    "# from our Ubuntu corpus.\n",
    "\n",
    "%time lda_model = gensim.models.LdaModel(ubuntu_corpus, num_topics=25, id2word=id2word_ubuntu, passes=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: LDA Top 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prettify_topic_representations(lda, corpus, num_topics, top_n):\n",
    "    \"\"\"\n",
    "    Print representations of the top_n terms of the topic along\n",
    "    with the coherence for each topic\n",
    "    \"\"\"\n",
    "    top_topics = lda.top_topics(corpus, topn=top_n)[:num_topics]\n",
    "    for i, t in tqdm(enumerate(top_topics)):\n",
    "        topic_repr, coherence_score = t\n",
    "        top_words = [t[1] for t in topic_repr]\n",
    "        print \"Topic: {}\".format(i+1)\n",
    "        print \"Topic Representations: {}\".format(\" | \".join(top_words))\n",
    "        print \"Coherence Score: {0:0.3f}\".format(coherence_score)\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 3321.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "Topic Representations: windows | partition | drive | gb | xp | want | hard | linux | disk | boot\n",
      "Coherence Score: -2.303\n",
      "\n",
      "Topic: 2\n",
      "Topic Representations: server | network | wireless | connect | internet | ip | connection | router | using | set\n",
      "Coherence Score: -2.410\n",
      "\n",
      "Topic: 3\n",
      "Topic Representations: apt | package | sudo | list | packages | update | remove | installed | synaptic | manager\n",
      "Coherence Score: -2.418\n",
      "\n",
      "Topic: 4\n",
      "Topic Representations: linux | work | would | well | ve | one | windows | think | really | good\n",
      "Coherence Score: -2.496\n",
      "\n",
      "Topic: 5\n",
      "Topic Representations: cd | usb | live | dev | iso | drive | mount | dvd | image | boot\n",
      "Coherence Score: -2.550\n",
      "\n",
      "Topic: 6\n",
      "Topic Representations: question | someone | ask | channel | please | hi | hello | tell | problem | guys\n",
      "Coherence Score: -2.573\n",
      "\n",
      "Topic: 7\n",
      "Topic Representations: thanks | good | program | looking | way | want | would | software | something | one\n",
      "Coherence Score: -2.594\n",
      "\n",
      "Topic: 8\n",
      "Topic Representations: gnome | desktop | click | right | window | kde | default | menu | manager | want\n",
      "Coherence Score: -2.641\n",
      "\n",
      "Topic: 9\n",
      "Topic Representations: command | file | sudo | root | user | etc | line | password | folder | home\n",
      "Coherence Score: -2.861\n",
      "\n",
      "Topic: 10\n",
      "Topic Representations: http | com | org | www | php | java | html | google | page | ubuntuforums\n",
      "Coherence Score: -2.881\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prettify_topic_representations(lda_model, ubuntu_corpus, num_topics=10, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Semantic Transformation: Tfidf + Latent Semantic Analysis\n",
    "\n",
    "Here we'll train a TFIDF model, and then train Latent Semantic Analysis on top of TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 145 ms, total: 23.1 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(ubuntu_corpus, id2word=id2word_ubuntu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 4 s, total: 1min 31s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[ubuntu_corpus], id2word=id2word_ubuntu, num_topics=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: LSA Top 10 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prettify_lsa_top_topics(lsa, num_topics, num_words):\n",
    "    \"\"\"\n",
    "    Print LSA's n most significant topics where n = num_topics.\n",
    "    For each topic, print the topic's most m signficant words where m = num_words.\n",
    "    \"\"\"\n",
    "    top_topics = lsa.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "    for idx, topic_repr in top_topics:\n",
    "        top_words = [t[0] for t in topic_repr]\n",
    "        print \"Topic: {}\".format(idx+1)\n",
    "        print \"Topic Representations: {}\".format(\" | \".join(top_words))\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "Topic Representations: windows | apt | sudo | hi | file | linux | want | one | installed | cd\n",
      "\n",
      "Topic: 2\n",
      "Topic Representations: apt | sudo | windows | grub | boot | cd | partition | package | drive | paste\n",
      "\n",
      "Topic: 3\n",
      "Topic Representations: paste | http | com | please | punctuation | flood | sudo | enter | apt | root\n",
      "\n",
      "Topic: 4\n",
      "Topic Representations: root | grub | sudo | boot | paste | password | partition | bit | card | cd\n",
      "\n",
      "Topic: 5\n",
      "Topic Representations: apt | root | grub | password | upgrade | update | user | boot | package | cd\n",
      "\n",
      "Topic: 6\n",
      "Topic Representations: grub | nvidia | drivers | card | driver | xorg | boot | cd | conf | ati\n",
      "\n",
      "Topic: 7\n",
      "Topic Representations: gnome | kde | desktop | kubuntu | root | ask | sudo | linux | menu | grub\n",
      "\n",
      "Topic: 8\n",
      "Topic Representations: ask | grub | question | file | channel | hello | hi | nvidia | xorg | please\n",
      "\n",
      "Topic: 9\n",
      "Topic Representations: cd | windows | live | grub | file | upgrade | root | iso | linux | wine\n",
      "\n",
      "Topic: 10\n",
      "Topic Representations: file | bit | windows | ask | root | cd | linux | question | hi | password\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prettify_lsa_top_topics(lsa=lsi_model, num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Discussion\n",
    "\n",
    "As we can see above, LDA provides more human interpretable topics than LSA. We can further evaluate the quality of our LDA model. This will be done later in the notebook.\n",
    "\n",
    "By inspecting the top 10 topic reprsentations of our LDA model, we think the topics are the following:\n",
    "    1. Partitioning Ubuntu Hard Drive With Windows XP or other Windows OS.\n",
    "    2. Network / Wifi/ Internet Related Issues\n",
    "    3. Apt Command Issues \n",
    "    4. Linux Suggestions\n",
    "    5. Ubuntu Device Mounting\n",
    "    6. Questions Asking for Help to Resolve Technical Problem (Too generalizable)\n",
    "    7. Software Recommendation\n",
    "    8. Linux Desktop Enviornments\n",
    "    9. Root User Privileges\n",
    "    10. Web Related Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Unseen Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'add', 1), (u'installed', 1), (u'apt', 1), (u'without', 1), (u'web', 2), (u'see', 1), (u'let', 1), (u'download', 1), (u'want', 1), (u'remove', 1), (u'already', 1), (u'everything', 2), (u'take', 1), (u'program', 2), (u'downloaded', 1), (u'var', 1), (u'cache', 1), (u'programs', 1), (u'archives', 1), (u'indeed', 1), (u'aptoncd', 1), (u'stored', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = extract_text_from_chat_dialogue('data/dialogs/5/100003.tsv')\n",
    "\n",
    "bow_vector = id2word_ubuntu.doc2bow(tokenize(text))\n",
    "print [(id2word_ubuntu[id], count) for id, count in bow_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prettify_model_vector_relevant_topics(model, vector, num_topics):\n",
    "    \"\"\"\n",
    "    Print the n most signifcant topics of a model vector\n",
    "    where n = num_topics.\n",
    "    \"\"\"\n",
    "    most_prominent_topics = list(sorted(vector, key=lambda t: t[1], reverse=True))[:num_topics]\n",
    "    for idx, t in enumerate(most_prominent_topics):\n",
    "        topic_repr = model.show_topic(t[0])\n",
    "        top_words = [t[0] for t in topic_repr]\n",
    "        print \"Topic: {}\".format(idx+1)\n",
    "        print \"Topic Representations: {}\".format(\" | \".join(top_words))\n",
    "        print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "Topic Representations: apt | package | sudo | list | packages | update | remove | installed | synaptic | manager\n",
      "\n",
      "Topic: 2\n",
      "Topic Representations: thanks | good | program | looking | way | want | would | software | something | one\n",
      "\n",
      "Topic: 3\n",
      "Topic Representations: terminal | open | ssh | run | nautilus | port | file | virtualbox | virtual | machine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform into LDA space\n",
    "lda_vector = lda_model[bow_vector]\n",
    "\n",
    "# print the document's top 3 most prominent LDA topics\n",
    "prettify_model_vector_relevant_topics(lda_model, lda_vector, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 1\n",
      "Topic Representations: windows | apt | sudo | hi | file | linux | want | one | installed | cd\n",
      "\n",
      "Topic: 2\n",
      "Topic Representations: work | im | update | using | package | installed | remove | doesn | log | see\n",
      "\n",
      "Topic: 3\n",
      "Topic Representations: apt | root | grub | password | upgrade | update | user | boot | package | cd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform into LSI space\n",
    "lsi_vector = lsi_model[tfidf_model[bow_vector]]\n",
    "\n",
    "# print the document's top 3 most prominent LSI topic (not interpretable like LDA!)\n",
    "prettify_model_vector_relevant_topics(lsi_model, lsi_vector, num_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Evaluation\n",
    "\n",
    "Our topic models are unsupervised models. Thus, we do not know apriori knowledge of what the topics ought to look like. This makes evalution difficult. Unlike in supervised learning where we simply compare predicted labels to actual labels, there are no labels in topic modelling.\n",
    "\n",
    "Each topic model (LDA, LSA) has its own way of measuring internal quality of its predictions. The best way to evaluate quality of unsupervised taks is to evaluate how they improve the superior task, the one we're actually training them for.\n",
    "\n",
    "For example, when the ultimate goal is retrieve semantically similar documents, we manually tag a set of similar documents and then see how a given semantic model maps those similar documents together.\n",
    "\n",
    "For our evaluation task, we will use a semi-automated task to evaluate the quality of our topic models. We'll split each document in our test into two parts, and check that:\n",
    "    1. Topics of the first half are similar to topics of the second.\n",
    "    2. Halves of different documents are mostly dissimilar.\n",
    "The similarity metric we will be using for our model evaluation is cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on 30K documents not used in LDA / LSA training.\n",
    "# we will be using the chat dialogs found in the dialogs/5 folder\n",
    "test_chat_files = glob.glob('data/dialogs/5/*.tsv')\n",
    "test_doc_stream = (tokens for tokens in iter_ubuntu(test_chat_files))\n",
    "test_docs = list(itertools.islice(test_doc_stream, 5000, 35000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_docs, num_pairs=100000):\n",
    "    # split each test document into two halves and compute topics for each half\n",
    "    part1 = [model[id2word_ubuntu.doc2bow(tokens[: len(tokens) / 2])] for tokens in test_docs]\n",
    "    part2 = [model[id2word_ubuntu.doc2bow(tokens[len(tokens) / 2 :])] for tokens in test_docs]\n",
    "    \n",
    "    # print computed similarities (uses cossim)\n",
    "    similarity_1 = np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)])\n",
    "    print \"Average cosine similarity between corresponding parts (higher is better): {}\".format(similarity_1)\n",
    "\n",
    "    random_pairs = np.random.randint(0, len(test_docs), size=(num_pairs, 2))\n",
    "    similarity_2 = np.mean([gensim.matutils.cossim(part1[i[0]], part2[i[1]]) for i in random_pairs])\n",
    "    print \"Average cosine similarity between 100,000 random parts (lower is better): {}\".format(similarity_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity between corresponding parts (higher is better): 0.426623166567\n",
      "Average cosine similarity between 100,000 random parts (lower is better): 0.178994328871\n",
      "CPU times: user 56.8 s, sys: 192 ms, total: 57 s\n",
      "Wall time: 57.1 s\n"
     ]
    }
   ],
   "source": [
    "%time evaluate_model(lda_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity between corresponding parts (higher is better): 0.259041220771\n",
      "Average cosine similarity between 100,000 random parts (lower is better): 0.0540667903812\n",
      "CPU times: user 25.2 s, sys: 558 ms, total: 25.7 s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%time evaluate_model(lsi_model, test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Persistence\n",
    "\n",
    "Save models to disk, so it can be re-used later (or be used on a different computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model.save('./data/gensim_models/lda_ubuntu.model')\n",
    "lsi_model.save('./data/gensim_models/lsa_ubuntu.model')\n",
    "tfidf_model.save('./data/gensim_models/tfidf_ubuntu.model')\n",
    "id2word_ubuntu.save('./data/gensim_models/ubuntu.dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Create A Topic Detector\n",
    "\n",
    "Since our LSA model has the best evaluation metrics, we now write a topic detector using our LSA topic model to generate a set of relevant topics for a given conversation (.tsv file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "class UbuntuTopicDetector(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize our LDA model, gensim Dictionary object, and array for STOPWORDs.\n",
    "        \"\"\"\n",
    "        self.model = gensim.models.LdaModel.load('./data/gensim_models/lda_ubuntu.model')\n",
    "        self.id2word_ubuntu = gensim.corpora.Dictionary.load('./data/gensim_models/ubuntu.dictionary')\n",
    "        self.STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "    def extract_text_from_chat_dialogue(self, chat_file):\n",
    "        \"\"\"\n",
    "        Extract Text from chat .tsv file.\n",
    "        \"\"\"\n",
    "        chat_dialogue = self.read_tsv_file(chat_file)\n",
    "        return ' '.join(chat_dialogue)\n",
    "    \n",
    "    def predict_topics(self, chat_file, top_n=3):\n",
    "        \"\"\"\n",
    "        Predict n most relevant topics for a given\n",
    "        chat file where n = top_n.\n",
    "        \"\"\"\n",
    "        bow_vector = self.vectorize_document(chat_file)\n",
    "        lda_vector = self.transform_into_semantic_space(bow_vector)\n",
    "        print \"Chat File: {}\".format(chat_file)\n",
    "        self.print_relevant_topics(lda_vector, num_topics=top_n)\n",
    "\n",
    "    def print_relevant_topics(self, vector, num_topics):\n",
    "        \"\"\"\n",
    "        Print the n most signifcant topics of a model vector\n",
    "        where n = num_topics.\n",
    "        \"\"\"\n",
    "        most_prominent_topics = list(sorted(vector, key=lambda t: t[1], reverse=True))[:num_topics]\n",
    "        for idx, t in enumerate(most_prominent_topics):\n",
    "            topic_repr = self.model.show_topic(t[0])\n",
    "            top_words = [t[0] for t in topic_repr]\n",
    "            print \"Topic: {}\".format(idx+1)\n",
    "            print \"Topic Representations: {}\".format(\" | \".join(top_words))\n",
    "            print\n",
    "        \n",
    "    def read_tsv_file(self, chat_file):\n",
    "        \"\"\"\n",
    "        Extract text from each row in the chat log file\n",
    "        and return a list of dialogue containing the\n",
    "        entire conversation.\n",
    "        \"\"\"\n",
    "        dialogue = []\n",
    "        with open(chat_file) as tsv_file:\n",
    "            reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                dialogue.append(str(row[3]).strip())\n",
    "        return dialogue\n",
    "    \n",
    "    def transform_into_semantic_space(self, vector):\n",
    "        \"\"\"\n",
    "        Transform bag of words vector into LDA semantic\n",
    "        space.\n",
    "        \"\"\"\n",
    "        lda_vector = self.model[vector]\n",
    "        return lda_vector\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Remove punctuation and lowercase text, then\n",
    "        generate tokens of our chat file.\n",
    "        \"\"\"\n",
    "        return [token for token in simple_preprocess(strip_punctuation(text.strip())) if token not in self.STOPWORDS]\n",
    "    \n",
    "    def vectorize_document(self, chat_file):\n",
    "        \"\"\"\n",
    "        Vectorize document into a bag of words vector.\n",
    "        \"\"\"\n",
    "        text = self.extract_text_from_chat_dialogue(chat_file)\n",
    "        bow_vector = self.id2word_ubuntu.doc2bow(self.tokenize(text))\n",
    "        return bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_detector = UbuntuTopicDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat File: data/dialogs/5/100000.tsv\n",
      "Topic: 1\n",
      "Topic Representations: kernel | source | files | find | delete | file | make | linux | code | build\n",
      "\n",
      "Topic: 2\n",
      "Topic Representations: command | file | sudo | root | user | etc | line | password | folder | home\n",
      "\n",
      "Topic: 3\n",
      "Topic Representations: http | com | org | www | php | java | html | google | page | ubuntuforums\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the 3 most relvant topics for a file in 'dialogs/5' folder\n",
    "topic_detector.predict_topics(test_chat_files[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat File: data/dialogs/5/100004.tsv\n",
      "Topic: 1\n",
      "Topic Representations: grub | error | boot | installed | problem | trying | fix | message | getting | usr\n",
      "\n",
      "Topic: 2\n",
      "Topic Representations: thanks | good | program | looking | way | want | would | software | something | one\n",
      "\n",
      "Topic: 3\n",
      "Topic Representations: screen | system | alt | back | restart | problem | panel | settings | ctrl | terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_detector.predict_topics(test_chat_files[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
